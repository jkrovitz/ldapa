{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "import pickle\n",
    "import bs4\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(driver, city, state):\n",
    "    textbox = driver.find_element_by_xpath('//input[@class=\"large has-menu\"]')\n",
    "    textbox.click()\n",
    "    textbox.send_keys(str(city)+', '+str(state), Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_page(driver):\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source)\n",
    "    links = soup.findAll('a', {'class': 'result-name'})\n",
    "    hrefs = []\n",
    "    for l in links:\n",
    "        string = str(l)\n",
    "        entry = re.findall('(?<=href=\\\")(.*)(?=\\\" item)', string)\n",
    "        hrefs.append(entry[0])\n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_page(driver):\n",
    "    nextpage=driver.find_element_by_xpath('//a[@class=\"btn btn-default btn-next\"]')\n",
    "    if nextpage:\n",
    "        print('Moving to next page ...')\n",
    "        nextpage.click()\n",
    "    else:\n",
    "        print('No more pages ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagination(driver):\n",
    "    link_text=get_links_from_page(driver)\n",
    "    next_page(driver)\n",
    "    return link_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_links():\n",
    "    results=[]\n",
    "    chrome_option=ChromeOptions()\n",
    "    chrome_option.add_argument('--headless')\n",
    "    driver=Chrome(r'/Users/jkrovitz/Documents/LDA_OF_PA_Project/ldapa/driver/chromedriver', options=chrome_option)\n",
    "    driver.get('https://www.psychologytoday.com/us')\n",
    "    search(driver, 'Pittsburgh', 'PA')\n",
    "    while True:\n",
    "        try:\n",
    "            entry=pagination(driver)\n",
    "            results.append(entry)\n",
    "        except StaleElementReferenceException as e:\n",
    "            print('Error:', e)\n",
    "            continue\n",
    "        except NoSuchElementException as e:\n",
    "            print('No more pages ...')\n",
    "            break\n",
    "    driver.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_entry(entry):\n",
    "    proper_keys=['name', 'phone', 'city', 'state', 'zip_code', 'information', 'personal_website']\n",
    "    missing_keys=list(set(proper_keys).difference(list(entry.keys())))\n",
    "    if missing_keys:\n",
    "        for k in missing_keys:\n",
    "            entry.update({k:'None'})\n",
    "    entry_sorted=dict(collections.OrderedDict(entry))\n",
    "    return entry_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pages(list_of_links):\n",
    "    result_list=[]\n",
    "    chrome_option=ChromeOptions()\n",
    "    chrome_option.add_argument('--headless')\n",
    "    driver=Chrome(r'/Users/jkrovitz/Documents/LDA_OF_PA_Project/ldapa/driver/chromedriver', options=chrome_option)\n",
    "    for l in list_of_links:\n",
    "        result={}\n",
    "        driver.get(l)\n",
    "        soup=BeautifulSoup(driver.page_source)\n",
    "        name=soup.find('h1', {'itemprop':'name'})\n",
    "        if name:\n",
    "            result['name']=name.text\n",
    "        else:\n",
    "            result['name']='None'\n",
    "        phone=soup.find('a',{'data-event-label':'Profile_PhoneLink'})\n",
    "        if phone:\n",
    "            result['phone']=phone.text\n",
    "        else:\n",
    "            result['phone']='None'\n",
    "        city=soup.find('span',{'itemprop':'addressLocality'})\n",
    "        if city:\n",
    "            result['city']=city.text.replace(',','')\n",
    "        else:\n",
    "            result['city']='None'\n",
    "        state=soup.find('span',{'itemprop':'addressRegion'})\n",
    "        if state:\n",
    "            result['state']=state.text\n",
    "        else:\n",
    "            result['state']='None'\n",
    "        zip_code=soup.find('span',{'itemprop':'postalcode'})\n",
    "        if zip_code:\n",
    "            result['zip_code']=zip_code.text\n",
    "        else:\n",
    "            result['zip_code']='None'\n",
    "        specalites=soup.findAll('li', {'class':'highlight'})\n",
    "        if specalites:\n",
    "            spec_text = [s.text.strip() for s in specalites]\n",
    "        try:\n",
    "            issues=soup.findAll('ul',{'class':'attribute-list copy-small'})[1]\n",
    "            if issues:\n",
    "                issues_text=[i.text.strip() for i in issues if type(i)==bs4.element.Tag]\n",
    "        except IndexError:\n",
    "            issues_text=[]\n",
    "            continue\n",
    "        try:\n",
    "            mental_health=soup.findAll('ul',{'class':'attribute-list copy-small'})[2]\n",
    "            if mental_health:\n",
    "                mental_text=[i.text.strip() for i in mental_health if type(i)==bs4.element.Tag]\n",
    "        except IndexError:\n",
    "            mental_text=[]\n",
    "            continue\n",
    "        issues_2=soup.findAll('ul', {'class':'attribute-list copy-small'})\n",
    "        if issues_2:\n",
    "            issues_2_text=[i.text.strip() for i in issues_2 if type(i)==bs4.element.Tag]\n",
    "            if issues_2_text:\n",
    "                clean_text=[t.replace('\\n', ',') for t in issues_2_text]\n",
    "                split_text=[t.split(',') for t in clean_text]\n",
    "                issues_2_text_clean=[]\n",
    "                for t in split_text:\n",
    "                    for r in t:\n",
    "                        if len(r) > 0 and any(c.isalpha() for c in r):\n",
    "                            issues_2_text_clean.append(r.strip())\n",
    "        full_specs=list(set(spec_text+issues_text+issues_2_text_clean+mental_text))\n",
    "        if len(full_specs)==0:\n",
    "            full_specs=['None']\n",
    "        result['information']=full_specs\n",
    "        website_text=soup.find('a', {'data-event-label': 'links-website'})\n",
    "        if website_text:\n",
    "            if website_text.has_attr('href')=='True':\n",
    "                driver.get(website_text.attrs['href'])\n",
    "                personal_website=driver.current_url\n",
    "                result['personal_website']=personal_website\n",
    "        else:\n",
    "            result['personal_website']='None'\n",
    "        checked_result=check_entry(result)\n",
    "        result_list.append(checked_result)\n",
    "    driver.close()\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "Moving to next page ...\n",
      "No more pages ...\n"
     ]
    }
   ],
   "source": [
    "r=collect_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for sublist in r:\n",
    "    for item in sublist:\n",
    "        final_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_parse=list(set(final_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_to_enter=parse_pages(list_to_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_to_enter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/pitt_data.txt\", \"wb\") as fp:\n",
    "    pickle.dump(data_to_enter, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
